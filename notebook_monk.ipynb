{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Monk Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "\n",
    "problem   = \"monks-3\"\n",
    "dataset_path  = f\"datasets/monks/{problem}\"\n",
    "\n",
    "train       = \".train\"\n",
    "test        = \".test\"\n",
    "\n",
    "f = open(dataset_path + train, 'r')\n",
    "res = f.readlines()\n",
    "f.close()\n",
    "train_str = ''.join(res)\n",
    "\n",
    "f = open(dataset_path + test, 'r')\n",
    "res = f.readlines()\n",
    "f.close()\n",
    "test_str = ''.join(res)\n",
    "\n",
    "def retrieveData(data_str):\n",
    "    # Create a DataFrame from the structured data\n",
    "    encoding_length = []\n",
    "    column_names = [\"R\", \"col2\", \"col3\", \"col4\", \"col5\", \"col6\", \"col7\", \"data\"]\n",
    "    column_features = [\"col2\", \"col3\", \"col4\", \"col5\", \"col6\", \"col7\"]\n",
    "    data = pd.read_csv(StringIO(data_str), sep=' ', header=None, names=column_names)\n",
    "    data = data.iloc[:, :-1]\n",
    "    for col in column_features:\n",
    "        encoding_length.append(max(data[col].unique()))\n",
    "    data=data.iloc[np.random.permutation(len(data))]\n",
    "    \n",
    "    #scaler = MinMaxScaler()\n",
    "    #df_scaled = scaler.fit_transform(data.to_numpy())\n",
    "    df_scaled = pd.DataFrame(data.to_numpy(), columns=data.columns.values)\n",
    "    del df_scaled['R']\n",
    "    #del df_scaled['Id']\n",
    "    df_scaled = df_scaled.assign(R=data['R'].values)\n",
    "    df_train = df_scaled\n",
    "    \n",
    "    features = 6\n",
    "    X_train = df_train.iloc[ : , :features].values\n",
    "    y_train = df_train.iloc[:,features:].values\n",
    "    return X_train, y_train, encoding_length\n",
    "\n",
    "def oneHotEncoding(X_data, l):\n",
    "    X_result = []\n",
    "    for x in X_data:\n",
    "        p = []\n",
    "        for i in range(len(x)):\n",
    "            d = [0] * l[i]\n",
    "            if x[i] == 1:\n",
    "                d[0] = 1\n",
    "            elif x[i] == 2:\n",
    "                d[1] = 1\n",
    "            elif x[i] == 3:\n",
    "                d[2] = 1\n",
    "            elif x[i] == 4:\n",
    "                d[3] = 1\n",
    "            p += d\n",
    "        X_result.append(p)\n",
    "    return X_result\n",
    "\n",
    "X_train, y_train, encoding_length = retrieveData(train_str)\n",
    "X_train = oneHotEncoding(X_train, encoding_length)\n",
    "\n",
    "X_test, y_test, encoding_length = retrieveData(test_str)\n",
    "X_test = oneHotEncoding(X_test, encoding_length)\n",
    "\n",
    "print(\"input len: \", len(X_train[0]))\n",
    "print(\"output len: \", len(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Selection - Hold-out Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from activation_function import instantiate_act_func\n",
    "from layer import Layer\n",
    "from mlp import MLP\n",
    "from losses import instantiate_loss\n",
    "from grid_search import create_test\n",
    "from weigth_init import instantiate_initializer\n",
    "from utils import k_fold_cross_validation, hold_out_validation\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path  = f\"models/{problem}\"\n",
    "\n",
    "json_file_config = [\n",
    "    f\"{model_path}/model1.json\",\n",
    "]\n",
    "tests = create_test(json_file_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_from_test(test):\n",
    "    layers = []\n",
    "    for layer in test['layers']:\n",
    "        layers.append(\n",
    "            Layer(\n",
    "                layer['units'],\n",
    "                instantiate_act_func(layer['act_func']),\n",
    "                layer['inputs'],\n",
    "                weights_initializer=instantiate_initializer(test['weights_initializer']),\n",
    "                kernel_regularizer=test['kernel_regularizer'],\n",
    "                bias_regularizer=test['bias_regularizer'],\n",
    "                momentum=test['momentum'],\n",
    "                Nesterov=test['Nesterov']\n",
    "            )\n",
    "        )\n",
    "    mlp = MLP(layers)\n",
    "    mlp.compile(test['learning_rate'],instantiate_loss(test['loss']), test['metrics'])\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(path, test, accuracy, summary):\n",
    "    iso_date = datetime.now().replace(microsecond=0).isoformat()\n",
    "    filename = f\"{path}/{iso_date}-acc:{str(round(accuracy, 2))}\"\n",
    "    f = open(f\"{filename}.logs\", 'w')\n",
    "    f.write(f\"{str(test)}\\n\")\n",
    "    f.write(f\"{summary}\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_chart(path, tr_res, vl_res, tr_label, vl_label, y_label):\n",
    "    iso_date = datetime.now().replace(microsecond=0).isoformat()\n",
    "    plt.plot(tr_res, label=tr_label, color='blue')\n",
    "    plt.plot(vl_res, label=vl_label, color='red', linestyle='--')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend()\n",
    "    filename = f\"{path}/{iso_date}-{y_label}\"\n",
    "    plt.savefig(f'{filename}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_selection_result = f\"results/model-selection/{problem}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "best_model = None\n",
    "best_accuracy = 0\n",
    "for test in tests:\n",
    "    epochs = round(test['epochs'] / k)\n",
    "    model = create_model_from_test(test)\n",
    "    dataset = hold_out_validation(X_train, y_train)\n",
    "    print(test)\n",
    "    tr_errors = []\n",
    "    vl_errors = []\n",
    "    tr_accuracies = []\n",
    "    vl_accuracies = []\n",
    "    for fold in dataset:\n",
    "        bar = trange(epochs)\n",
    "        for _ in bar:\n",
    "            tr_error, tr_accuracy = model.fit(fold['X_train'], fold['y_train'], 1)\n",
    "            vl_error, vl_accuracy = model.evaluate(fold['X_val'], fold['y_val'])\n",
    "            tr_errors.append(tr_error[0])\n",
    "            vl_errors.append(vl_error)\n",
    "            tr_accuracies.append(tr_accuracy[0])\n",
    "            vl_accuracies.append(vl_accuracy)\n",
    "            bar.set_description(f'(loss={vl_error})')\n",
    "        summary = model.summary()\n",
    "\n",
    "    if best_accuracy < vl_accuracy:\n",
    "        best_accuracy = vl_accuracy\n",
    "        best_model = test\n",
    "\n",
    "    save_result(path_model_selection_result, test, vl_accuracy, summary)\n",
    "    plot_chart(path_model_selection_result, tr_errors, vl_errors, \"Train Error\", \"Valid Error\", \"Error\")\n",
    "    plot_chart(path_model_selection_result, tr_accuracies, vl_accuracies, \"Train Accuracy\", \"Valid Accuracy\", \"Accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model  Assessment - Hold-out Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_assessment_result = f\"results/model-assessment/{problem}\"\n",
    "model = create_model_from_test(best_model)\n",
    "tr_errors = []\n",
    "vl_errors = []\n",
    "tr_accuracies = []\n",
    "vl_accuracies = []\n",
    "for i in range(best_model['epochs']):\n",
    "    tr_error, tr_accuracy = model.fit(X_train, y_train, 1)\n",
    "    vl_error, vl_accuracy = model.evaluate(X_test, y_test)\n",
    "    tr_errors.append(tr_error)\n",
    "    vl_errors.append(vl_error)\n",
    "    tr_accuracies.append(tr_accuracy[0])\n",
    "    vl_accuracies.append(vl_accuracy)\n",
    "summary = model.summary()\n",
    "\n",
    "save_result(path_model_assessment_result, best_model, vl_accuracy, summary)\n",
    "plot_chart(path_model_assessment_result, tr_errors, vl_errors, \"Train Error\", \"Valid Error\", \"Error\")\n",
    "plot_chart(path_model_assessment_result, tr_accuracies, vl_accuracies, \"Train Accuracy\", \"Valid Accuracy\", \"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(path_model_assessment_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
