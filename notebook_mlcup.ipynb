{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "problem         = \"ML-CUP23\"\n",
    "filename        = f\"datasets/ML-CUP/{problem}\"\n",
    "train           = \"-TR.csv\"\n",
    "test            = \"-TS.csv\"\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "def has_nan(lst):\n",
    "    return any(math.isnan(x) for x in lst)\n",
    "\n",
    "def reduce_target(y):\n",
    "    return scaler.fit_transform(y)\n",
    "\n",
    "def string_dataset_to_float(dataset):\n",
    "    new_dataset = []\n",
    "    for i in range(len(dataset)):\n",
    "        if has_nan(dataset[i]) == False:\n",
    "            new_dataset.append([float(j) for j in dataset[i]])\n",
    "    return np.array(new_dataset)\n",
    "\n",
    "def retrieveData(filename):\n",
    "    column_names    = [\"id\", \"i1\", \"i2\", \"i3\", \"i4\", \"i5\", \"i6\", \"i7\", \"i8\", \"i9\", \"i10\", \"t1\", \"t2\", \"t3\"]\n",
    "    column_features = [\"i1\", \"i2\", \"i3\", \"i4\", \"i5\", \"i6\", \"i7\", \"i8\", \"i9\", \"i10\"]\n",
    "    data = pd.read_csv(filename, sep=',', header=None, comment=\"#\", names=column_names)\n",
    "    data=data.iloc[np.random.permutation(len(data))]\n",
    "    column_names = column_names[1:]\n",
    "    df_scaled = pd.DataFrame(data.to_numpy(), columns=data.columns.values)\n",
    "    del df_scaled['id']\n",
    "    df_train = df_scaled\n",
    "    features = len(column_features)\n",
    "    X_train = df_train.iloc[ : , :features].values\n",
    "    y_train = df_train.iloc[:,features:].values\n",
    "    #print(column_names)\n",
    "    X_train = string_dataset_to_float(X_train)\n",
    "    y_train = string_dataset_to_float(y_train)\n",
    "    return X_train, y_train\n",
    "\n",
    "def oneHotEncoding(X_data, l):\n",
    "    X_result = []\n",
    "    for x in X_data:\n",
    "        p = []\n",
    "        for i in range(len(x)):\n",
    "            d = [0] * l[i]\n",
    "            if x[i] == 1:\n",
    "                d[0] = 1\n",
    "            elif x[i] == 2:\n",
    "                d[1] = 1\n",
    "            elif x[i] == 3:\n",
    "                d[2] = 1\n",
    "            elif x[i] == 4:\n",
    "                d[3] = 1\n",
    "            p += d\n",
    "        X_result.append(p)\n",
    "    return X_result\n",
    "\n",
    "X_train, y_train = retrieveData(filename + train)\n",
    "#X_test, y_test   = retrieveData(filename + test)\n",
    "y_train = reduce_target(y_train)\n",
    "y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from activation_function import instantiate_act_func\n",
    "from layer import Layer\n",
    "from mlp import MLP\n",
    "from losses import instantiate_loss\n",
    "from grid_search import create_test\n",
    "from weigth_init import instantiate_initializer\n",
    "from utils import k_fold_cross_validation, hold_out_validation\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_config = [\n",
    "    \"models/model1.json\",\n",
    "    # \"models/model2.json\",\n",
    "    # \"models/model3.json\",\n",
    "    # \"models/model4.json\",\n",
    "]\n",
    "tests = create_test(json_file_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_from_test(test):\n",
    "    layers = []\n",
    "    n_processes = None\n",
    "    for layer in test['layers']:\n",
    "        layers.append(\n",
    "            Layer(\n",
    "                layer['units'],\n",
    "                instantiate_act_func(layer['act_func']),\n",
    "                layer['inputs'],\n",
    "                weights_initializer=instantiate_initializer(test['weights_initializer']),\n",
    "                kernel_regularizer=test['kernel_regularizer'],\n",
    "                bias_regularizer=test['bias_regularizer'],\n",
    "                momentum=test['momentum'],\n",
    "                Nesterov=test['Nesterov'],\n",
    "                n_processes=n_processes\n",
    "            )\n",
    "        )\n",
    "    mlp = MLP(layers)\n",
    "    mlp.compile(test['learning_rate'],instantiate_loss(test['loss']), test['metrics'])\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(path, test, error, summary):\n",
    "    iso_date = datetime.now().replace(microsecond=0).isoformat()\n",
    "    filename = f\"{path}/{iso_date}-err:{str(round(error, 2))}\"\n",
    "    f = open(f\"{filename}.logs\", 'w')\n",
    "    f.write(f\"{str(test)}\\n\")\n",
    "    f.write(f\"{summary}\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_selection_result = f\"results/model-selection/{problem}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_errors(train_error, val_error):\n",
    "    plt.plot(train_error, label='Train Error', color='blue')\n",
    "    plt.plot(val_error, label='Val Error', color='red', linestyle='--')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('error')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{filename}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "k = 1\n",
    "best_test = None\n",
    "best_inst_model = None\n",
    "best_error = 1000\n",
    "for test in tests:\n",
    "    epochs = round(test['epochs'] / k)\n",
    "    model = create_model_from_test(test)\n",
    "    dataset = hold_out_validation(X_train, y_train)\n",
    "    print(test)\n",
    "    tr_errors = []\n",
    "    vl_errors = []\n",
    "    error = 0\n",
    "    for fold in dataset:\n",
    "        bar = trange(epochs, desc='ML')\n",
    "        for _ in bar:\n",
    "            tr_error = model.fit(fold['X_train'], fold['y_train'], 1)\n",
    "            vl_error = model.evaluate(fold['X_val'], fold['y_val'])\n",
    "            tr_errors.append(tr_error)\n",
    "            vl_errors.append(vl_error)\n",
    "            bar.set_description(f'ML (loss={vl_error})')\n",
    "        summary = model.summary()\n",
    "\n",
    "    plot_errors(tr_errors, vl_errors)\n",
    "    error = vl_errors[-1]\n",
    "\n",
    "    if best_error > error:\n",
    "        best_error = error\n",
    "        best_test = test\n",
    "        best_inst_model = model\n",
    "\n",
    "    save_result(path_model_selection_result, test, error, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_assessment_result = f\"results/model-assessment/{problem}\"\n",
    "model = create_model_from_test(best_test)\n",
    "from losses import MeanSquaredError\n",
    "l = MeanSquaredError()\n",
    "\n",
    "for fold in dataset:\n",
    "    error = 0\n",
    "    for x,y in zip(fold['X_train'], fold['y_train']):\n",
    "        out = best_inst_model.run(x)\n",
    "        print(scaler.inverse_transform([out]), scaler.inverse_transform([y]))\n",
    "        error += l.error(scaler.inverse_transform([out]), scaler.inverse_transform([y]))\n",
    "    print(error / len(fold['X_train']))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Assessment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_assessment_result = f\"results/model-assessment/{problem}\"\n",
    "model = create_model_from_test(best_test)\n",
    "\n",
    "errors = model.fit(X_train, y_train, best_test['epochs'])\n",
    "#error = model.evaluate(X_test, y_test)\n",
    "summary = model.summary()\n",
    "\n",
    "#save_result(path_model_assessment_result, best_model, error, errors, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = MeanSquaredError()\n",
    "global_error = 0\n",
    "for i in range(len(X_train)):\n",
    "    out = model.run(X_train[i])\n",
    "    print(scaler.inverse_transform([out]) - scaler.inverse_transform([y_train[i]]))\n",
    "    l.error(out, y_train[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
