{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from utils import hold_out_validation\n",
    "\n",
    "problem         = \"ML-CUP23\"\n",
    "filename        = f\"datasets/ML-CUP/{problem}\"\n",
    "train           = \"-TR.csv\"\n",
    "test            = \"-TS.csv\"\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "def has_nan(lst):\n",
    "    return any(math.isnan(x) for x in lst)\n",
    "\n",
    "def reduce_target(y):\n",
    "    return scaler.fit_transform(y)\n",
    "\n",
    "def string_dataset_to_float(dataset):\n",
    "    new_dataset = []\n",
    "    for i in range(len(dataset)):\n",
    "        if has_nan(dataset[i]) == False:\n",
    "            new_dataset.append([float(j) for j in dataset[i]])\n",
    "    return np.array(new_dataset)\n",
    "\n",
    "def retrieveData(filename, column_names, column_features):\n",
    "    data = pd.read_csv(filename, sep=',', header=None, comment=\"#\", names=column_names)\n",
    "    data=data.iloc[np.random.permutation(len(data))]\n",
    "    column_names = column_names[1:]\n",
    "    df_scaled = pd.DataFrame(data.to_numpy(), columns=data.columns.values)\n",
    "    del df_scaled['id']\n",
    "    df_train = df_scaled\n",
    "    features = len(column_features)\n",
    "    X_train = df_train.iloc[ : , :features].values\n",
    "    y_train = df_train.iloc[:,features:].values\n",
    "    #print(column_names)\n",
    "    X_train = string_dataset_to_float(X_train)\n",
    "    y_train = string_dataset_to_float(y_train)\n",
    "    return X_train, y_train\n",
    "\n",
    "def oneHotEncoding(X_data, l):\n",
    "    X_result = []\n",
    "    for x in X_data:\n",
    "        p = []\n",
    "        for i in range(len(x)):\n",
    "            d = [0] * l[i]\n",
    "            if x[i] == 1:\n",
    "                d[0] = 1\n",
    "            elif x[i] == 2:\n",
    "                d[1] = 1\n",
    "            elif x[i] == 3:\n",
    "                d[2] = 1\n",
    "            elif x[i] == 4:\n",
    "                d[3] = 1\n",
    "            p += d\n",
    "        X_result.append(p)\n",
    "    return X_result\n",
    "\n",
    "column_names    = [\"id\", \"i1\", \"i2\", \"i3\", \"i4\", \"i5\", \"i6\", \"i7\", \"i8\", \"i9\", \"i10\", \"t1\", \"t2\", \"t3\"]\n",
    "column_features = [\"i1\", \"i2\", \"i3\", \"i4\", \"i5\", \"i6\", \"i7\", \"i8\", \"i9\", \"i10\"]\n",
    "X_train, y_train = retrieveData(filename + train, column_names, column_features)\n",
    "y_train = reduce_target(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = hold_out_validation(X_train, y_train)[0]\n",
    "X_train, y_train, X_test, y_test = dataset[\"X_train\"], dataset['y_train'], dataset['X_val'], dataset['y_val']\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from activation_function import instantiate_act_func\n",
    "from layer import Layer\n",
    "from mlp import MLP\n",
    "from losses import instantiate_loss\n",
    "from grid_search import create_test\n",
    "from weigth_init import instantiate_initializer\n",
    "from utils import k_fold_cross_validation, hold_out_validation\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from losses import MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/ML-CUP\"\n",
    "json_file_config = [\n",
    "    f\"{model_path}/model1.json\",\n",
    "    f\"{model_path}/model2.json\",\n",
    "    f\"{model_path}/model3.json\",\n",
    "]\n",
    "tests = create_test(json_file_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_from_test(test):\n",
    "    layers = []\n",
    "    n_processes = None\n",
    "    for layer in test['layers']:\n",
    "        layers.append(\n",
    "            Layer(\n",
    "                layer['units'],\n",
    "                instantiate_act_func(layer['act_func']),\n",
    "                layer['inputs'],\n",
    "                weights_initializer=instantiate_initializer(test['weights_initializer']),\n",
    "                kernel_regularizer=test['kernel_regularizer'],\n",
    "                bias_regularizer=test['bias_regularizer'],\n",
    "                momentum=test['momentum'],\n",
    "                Nesterov=test['Nesterov'],\n",
    "                n_processes=n_processes\n",
    "            )\n",
    "        )\n",
    "    mlp = MLP(layers)\n",
    "    mlp.compile(test['learning_rate'],instantiate_loss(test['loss']), test['metrics'])\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(path, test, error, accuracy, summary):\n",
    "    iso_date = datetime.now().replace(microsecond=0).isoformat()\n",
    "    filename = f\"{path}/{iso_date}-err:{str(round(error, 6))}-mee:{str(round(accuracy, 6))}\"\n",
    "    f = open(f\"{filename}.logs\", 'w')\n",
    "    f.write(f\"{str(test)}\\n\")\n",
    "    f.write(f\"{summary}\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_selection_result = f\"results/model-selection/{problem}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_chart(path, tr_res, vl_res, tr_label, vl_label, y_label):\n",
    "    iso_date = datetime.now().replace(microsecond=0).isoformat()\n",
    "    plt.plot(tr_res, label=tr_label, color='blue')\n",
    "    plt.plot(vl_res, label=vl_label, color='red', linestyle='--')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend()\n",
    "    filename = f\"{path}/{iso_date}-{y_label}\"\n",
    "    plt.savefig(f'{filename}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "best_test = None\n",
    "best_inst_model = None\n",
    "best_error = 1000\n",
    "for test in tests:\n",
    "    epochs = test['epochs']\n",
    "    model = create_model_from_test(test)\n",
    "    dataset = hold_out_validation(X_train, y_train)\n",
    "    print(test)\n",
    "    tr_errors = []\n",
    "    vl_errors = []\n",
    "    tr_accuracies = []\n",
    "    vl_accuracies = []\n",
    "    for fold in dataset:\n",
    "        bar = trange(epochs)\n",
    "        for _ in bar:\n",
    "            tr_error, tr_accuracy = model.fit(fold['X_train'], fold['y_train'], 1)\n",
    "            vl_error, vl_accuracy = model.evaluate(fold['X_val'], fold['y_val'])\n",
    "            tr_errors.append(tr_error[0])\n",
    "            vl_errors.append(vl_error)\n",
    "            tr_accuracies.append(tr_accuracy[0])\n",
    "            vl_accuracies.append(vl_accuracy)\n",
    "            bar.set_description(f'(loss={vl_error})')\n",
    "        summary = model.summary()\n",
    "\n",
    "\n",
    "    error = vl_errors[-1]\n",
    "    accuracy = vl_accuracies[-1]\n",
    "\n",
    "    if best_error > error:\n",
    "        best_error = error\n",
    "        best_test = test\n",
    "        best_inst_model = model\n",
    "\n",
    "    save_result(path_model_selection_result, test, error, accuracy, summary)\n",
    "    plot_chart(path_model_selection_result, tr_errors, vl_errors, \"Train Error\", \"Valid Error\", \"Error\")\n",
    "    plot_chart(path_model_selection_result, tr_accuracies, vl_accuracies, \"Train MEE\", \"Valid MEE\", \"MEE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Assessment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_assessment_result = f\"results/model-assessment/{problem}\"\n",
    "model = create_model_from_test(best_test)\n",
    "tr_errors = []\n",
    "vl_errors = []\n",
    "tr_accuracies = []\n",
    "vl_accuracies = []\n",
    "bar = trange(best_test['epochs'])\n",
    "for _ in bar:\n",
    "    tr_error, tr_accuracy = model.fit(X_train, y_train, 1)\n",
    "    vl_error, vl_accuracy = model.evaluate(X_test, y_test)\n",
    "    tr_errors.append(tr_error[0])\n",
    "    vl_errors.append(vl_error)\n",
    "    tr_accuracies.append(tr_accuracy[0])\n",
    "    vl_accuracies.append(vl_accuracy)\n",
    "summary = model.summary()\n",
    "\n",
    "save_result(path_model_assessment_result, best_test, vl_error, vl_accuracy, summary)\n",
    "plot_chart(path_model_assessment_result, tr_errors, vl_errors, \"Train Error\", \"Valid Error\", \"Error\")\n",
    "plot_chart(path_model_assessment_result, tr_accuracies, vl_accuracies, \"Train MEE\", \"Valid MEE\", \"MEE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(path_model_assessment_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses import MeanEuclideanError\n",
    "l = MeanEuclideanError()\n",
    "global_error = 0\n",
    "global_error_transform = 0\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    out = model.run(X_train[i])\n",
    "    global_error += l.error(out, y_train[i])\n",
    "    global_error_transform += l.error(scaler.inverse_transform([out]), scaler.inverse_transform([y_train[i]]))\n",
    "print(global_error/len(X_train))\n",
    "print(global_error_transform/len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = MeanSquaredError()\n",
    "global_error = 0\n",
    "for i in range(len(X_test)):\n",
    "    out = model.run(X_test[i])\n",
    "    print(scaler.inverse_transform([out]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Compute Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from utils import hold_out_validation\n",
    "\n",
    "problem         = \"ML-CUP23\"\n",
    "filename        = f\"datasets/ML-CUP/{problem}\"\n",
    "test            = \"-TS.csv\"\n",
    "\n",
    "\n",
    "def has_nan(lst):\n",
    "    return any(math.isnan(x) for x in lst)\n",
    "\n",
    "def string_dataset_to_float(dataset):\n",
    "    new_dataset = []\n",
    "    for i in range(len(dataset)):\n",
    "        if has_nan(dataset[i]) == False:\n",
    "            new_dataset.append([float(j) for j in dataset[i]])\n",
    "    return np.array(new_dataset)\n",
    "\n",
    "def retrieveData(filename, column_names, column_features):\n",
    "    data = pd.read_csv(filename, sep=',', header=None, comment=\"#\", names=column_names)\n",
    "    data=data.iloc[np.random.permutation(len(data))]\n",
    "    df_scaled = pd.DataFrame(data.to_numpy(), columns=data.columns.values)\n",
    "    df_train = df_scaled\n",
    "    X_train = df_train.iloc[ : , :].values\n",
    "    X_train = string_dataset_to_float(X_train)\n",
    "    return X_train\n",
    "\n",
    "def oneHotEncoding(X_data, l):\n",
    "    X_result = []\n",
    "    for x in X_data:\n",
    "        p = []\n",
    "        for i in range(len(x)):\n",
    "            d = [0] * l[i]\n",
    "            if x[i] == 1:\n",
    "                d[0] = 1\n",
    "            elif x[i] == 2:\n",
    "                d[1] = 1\n",
    "            elif x[i] == 3:\n",
    "                d[2] = 1\n",
    "            elif x[i] == 4:\n",
    "                d[3] = 1\n",
    "            p += d\n",
    "        X_result.append(p)\n",
    "    return X_result\n",
    "\n",
    "column_names    = [\"id\", \"i1\", \"i2\", \"i3\", \"i4\", \"i5\", \"i6\", \"i7\", \"i8\", \"i9\", \"i10\"]\n",
    "column_features = [\"id\", \"i1\", \"i2\", \"i3\", \"i4\", \"i5\", \"i6\", \"i7\", \"i8\", \"i9\", \"i10\"]\n",
    "X_test = retrieveData(filename + test, column_names, column_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X_test:\n",
    "    print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team = \"team-name_ML-CUP23-TS.csv\"\n",
    "path_result = f\"results/model-assessment/ML-CUP23/{team}\"\n",
    "l = MeanSquaredError()\n",
    "f = open(path_result, \"w\")\n",
    "\n",
    "for x in X_test:\n",
    "    out = model.run(x[1:])\n",
    "    res = scaler.inverse_transform([out])\n",
    "    res = [str(i) for i in res[0]]\n",
    "    string = f\"{str(x[0])}, \" + \", \".join(res)\n",
    "    f.write(string + \"\\n\")\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
