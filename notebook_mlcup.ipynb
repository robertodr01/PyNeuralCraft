{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from utils import hold_out_validation\n",
    "\n",
    "problem         = \"ML-CUP23\"\n",
    "filename        = f\"datasets/ML-CUP/{problem}\"\n",
    "train           = \"-TR.csv\"\n",
    "test            = \"-TS.csv\"\n",
    "\n",
    "\n",
    "def has_nan(lst):\n",
    "    return any(math.isnan(x) for x in lst)\n",
    "\n",
    "def string_dataset_to_float(dataset):\n",
    "    new_dataset = []\n",
    "    for i in range(len(dataset)):\n",
    "        if has_nan(dataset[i]) == False:\n",
    "            new_dataset.append([float(j) for j in dataset[i]])\n",
    "    return np.array(new_dataset)\n",
    "\n",
    "def retrieveData(filename, column_names, column_features):\n",
    "    data = pd.read_csv(filename, sep=',', header=None, comment=\"#\", names=column_names)\n",
    "    data=data.iloc[np.random.permutation(len(data))]\n",
    "    column_names = column_names[1:]\n",
    "    df_scaled = pd.DataFrame(data.to_numpy(), columns=data.columns.values)\n",
    "    del df_scaled['id']\n",
    "    df_train = df_scaled\n",
    "    features = len(column_features)\n",
    "    X_train = df_train.iloc[ : , :features].values\n",
    "    y_train = df_train.iloc[:,features:].values\n",
    "    #print(column_names)\n",
    "    X_train = string_dataset_to_float(X_train)\n",
    "    y_train = string_dataset_to_float(y_train)\n",
    "    return X_train, y_train\n",
    "\n",
    "column_names    = [\"id\", \"i1\", \"i2\", \"i3\", \"i4\", \"i5\", \"i6\", \"i7\", \"i8\", \"i9\", \"i10\", \"t1\", \"t2\", \"t3\"]\n",
    "column_features = [\"i1\", \"i2\", \"i3\", \"i4\", \"i5\", \"i6\", \"i7\", \"i8\", \"i9\", \"i10\"]\n",
    "X_data, y_data = retrieveData(filename + train, column_names, column_features)\n",
    "dataset = hold_out_validation(X_data, y_data)[0]\n",
    "X_train = dataset[\"X_train\"]\n",
    "y_train = dataset[\"y_train\"]\n",
    "X_test = dataset[\"X_val\"]\n",
    "y_test = dataset[\"y_val\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from activation_function import instantiate_act_func\n",
    "from layer import Layer\n",
    "from mlp import MLP\n",
    "from losses import instantiate_loss\n",
    "from grid_search import create_test\n",
    "from weigth_init import instantiate_initializer\n",
    "from utils import hold_out_validation\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from losses import MeanEuclideanError, MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "filename = \"results/model-assessment/ML-CUP23/release/ML-CUP23.pkl\"\n",
    "res = []\n",
    "model = MLP()\n",
    "model.load(filename)\n",
    "mse, mse_norm = 0, 0\n",
    "mee, mee_norm = 0, 0\n",
    "mee_func, mse_func = MeanEuclideanError(), MeanSquaredError()\n",
    "\n",
    "for x, y in zip(X_train, y_train):\n",
    "    out = model.run(x)\n",
    "    mse += mse_func.error(y, out)\n",
    "    mee += mee_func.error(y, out)\n",
    "    res.append(out)\n",
    "\n",
    "y_train = scaler.inverse_transform(y_train)\n",
    "res = scaler.inverse_transform(res)\n",
    "\n",
    "for y, o in zip(y_train, res):\n",
    "    mse_norm += mse_func.error(y, o)\n",
    "    mee_norm += mee_func.error(y, o)\n",
    "print(\"mee: \", mee/len(y_train))\n",
    "print(\"mee norm: \", mee_norm/len(y_train))\n",
    "print(\"mse: \", mse/len(y_train))\n",
    "print(\"mse norm: \", mse_norm/len(y_train))\n",
    "''' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from activation_function import instantiate_act_func\n",
    "from layer import Layer\n",
    "from mlp import MLP\n",
    "from losses import instantiate_loss\n",
    "from grid_search import create_test\n",
    "from weigth_init import instantiate_initializer\n",
    "from utils import k_fold_cross_validation, hold_out_validation\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from losses import MeanEuclideanError, MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/ML-CUP\"\n",
    "json_file_config = [\n",
    "    f\"{model_path}/model1.json\",\n",
    "    f\"{model_path}/model2.json\",\n",
    "    f\"{model_path}/model3.json\",\n",
    "    f\"{model_path}/model4.json\",\n",
    "]\n",
    "tests = create_test(json_file_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_from_test(test):\n",
    "    layers = []\n",
    "    n_processes = None\n",
    "    for layer in test['layers']:\n",
    "        layers.append(\n",
    "            Layer(\n",
    "                layer['units'],\n",
    "                instantiate_act_func(layer['act_func']),\n",
    "                layer['inputs'],\n",
    "                weights_initializer=instantiate_initializer(test['weights_initializer']),\n",
    "                kernel_regularizer=test['kernel_regularizer'],\n",
    "                bias_regularizer=test['bias_regularizer'],\n",
    "                momentum=test['momentum'],\n",
    "                Nesterov=test['Nesterov'],\n",
    "                n_processes=n_processes\n",
    "            )\n",
    "        )\n",
    "    mlp = MLP(layers)\n",
    "    mlp.compile(test['learning_rate'],instantiate_loss(test['loss']), test['metrics'])\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(path, test, mse_train, mee_train, mse_val, mee_val, summary):\n",
    "    iso_date = datetime.now().replace(microsecond=0).isoformat()\n",
    "    rep_train = f\"mse_train:{str(round(mse_train, 6))}-mee_train:{str(round(mee_train, 6))}\"\n",
    "    rep_val = f\"mse_val:{str(round(mse_val, 6))}-mee_val:{str(round(mee_val, 6))}\"\n",
    "    filename = f\"{path}/{iso_date}-{rep_train}-{rep_val}\"\n",
    "    f = open(f\"{filename}.logs\", 'w')\n",
    "    f.write(f\"{str(test)}\\n\")\n",
    "    f.write(f\"{str(rep_train)}\\n\")\n",
    "    f.write(f\"{str(rep_val)}\\n\")\n",
    "    f.write(f\"{summary}\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_selection_result = f\"results/model-selection/{problem}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_chart(path, tr_res, vl_res, tr_label, vl_label, y_label):\n",
    "    iso_date = datetime.now().replace(microsecond=0).isoformat()\n",
    "    plt.plot(tr_res, label=tr_label, color='blue')\n",
    "    plt.plot(vl_res, label=vl_label, color='red', linestyle='--')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend()\n",
    "    filename = f\"{path}/{iso_date}-{y_label}\"\n",
    "    plt.savefig(f'{filename}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_f = MinMaxScaler()\n",
    "scaler_eval = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model, X, Y, scaler):\n",
    "    mse = 0\n",
    "    mee = 0\n",
    "    mee_func, mse_func = MeanEuclideanError(), MeanSquaredError()\n",
    "    outs = []\n",
    "    for x, _ in zip(X, Y):\n",
    "        out = model.run(x)\n",
    "        outs.append(out)\n",
    "    out_inv = scaler.inverse_transform(outs)\n",
    "    for out, y in zip(out_inv, Y):\n",
    "        mee += mee_func.error(out, y)\n",
    "        mse += mse_func.error(out, y)\n",
    "    return mse/len(Y), mee/len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "best_test = None\n",
    "best_inst_model = None\n",
    "best_error = 1000\n",
    "k = 5\n",
    "\n",
    "for test in tests:\n",
    "    tr_errors = []\n",
    "    vl_errors = []\n",
    "    tr_accuracies = []\n",
    "    vl_accuracies = []\n",
    "    epochs = round(test['epochs'] / k)\n",
    "    fold = k_fold_cross_validation(X_train, y_train, k)\n",
    "    print(test)\n",
    "    model = create_model_from_test(test)\n",
    "    for f in fold:\n",
    "        bar = trange(epochs)\n",
    "        y_train_norm = scaler_f.fit_transform(f['y_train'])\n",
    "        y_val_norm = scaler_eval.fit_transform(f['y_val'])\n",
    "        for i in bar:\n",
    "            tr_error, tr_accuracy = model.fit(f['X_train'], y_train_norm, 1)\n",
    "            vl_error, vl_accuracy = model.evaluate(f['X_val'], y_val_norm)\n",
    "            tr_errors.append(tr_error[0])\n",
    "            vl_errors.append(vl_error)\n",
    "            tr_accuracies.append(tr_accuracy[0])\n",
    "            vl_accuracies.append(vl_accuracy)\n",
    "            bar.set_description(f'(loss={vl_error})')\n",
    "        mse_train, mee_train = compute_metrics(model, f['X_train'],f['y_train'], scaler_f)\n",
    "        mse_val, mee_val = compute_metrics(model, f['X_val'],f['y_val'], scaler_eval)\n",
    "        \n",
    "    summary = model.summary()\n",
    "\n",
    "    error = vl_errors[-1]\n",
    "    accuracy = vl_accuracies[-1]\n",
    "    save_result(path_model_selection_result, test, mse_train, mee_train, mse_val, mee_val, summary)\n",
    "    plot_chart(path_model_selection_result, tr_errors, vl_errors, \"Train Error\", \"Valid Error\", \"Error\")\n",
    "    plot_chart(path_model_selection_result, tr_accuracies, vl_accuracies, \"Train MEE\", \"Valid MEE\", \"MEE\")\n",
    "\n",
    "    if best_error > error:\n",
    "        best_error = error\n",
    "        best_test = test\n",
    "        best_inst_model = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Assessment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_fit = MinMaxScaler()\n",
    "scaler_test = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model, X, Y, scaler):\n",
    "    mse = 0\n",
    "    mee = 0\n",
    "    mee_func, mse_func = MeanEuclideanError(), MeanSquaredError()\n",
    "    outs = []\n",
    "    for x, _ in zip(X, Y):\n",
    "        out = model.run(x)\n",
    "        outs.append(out)\n",
    "    out_inv = scaler.inverse_transform(outs)\n",
    "    for out, y in zip(out_inv, Y):\n",
    "        mee += mee_func.error(out, y)\n",
    "        mse += mse_func.error(out, y)\n",
    "    return mse/len(Y), mee/len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_assessment_result = f\"results/model-assessment/{problem}\"\n",
    "model = create_model_from_test(best_test)\n",
    "print(best_test)\n",
    "tr_errors = []\n",
    "vl_errors = []\n",
    "tr_accuracies = []\n",
    "vl_accuracies = []\n",
    "outs = []\n",
    "bar = trange(best_test['epochs'])\n",
    "for i in bar:\n",
    "    y_train_norm = scaler_fit.fit_transform(y_train)\n",
    "    y_test_norm = scaler_test.fit_transform(y_test)\n",
    "    tr_error, tr_accuracy = model.fit(X_train, y_train_norm, 1)\n",
    "    vl_error, vl_accuracy = model.evaluate(X_test, y_test_norm)\n",
    "    tr_errors.append(tr_error[0])\n",
    "    vl_errors.append(vl_error)\n",
    "    tr_accuracies.append(tr_accuracy[0])\n",
    "    vl_accuracies.append(vl_accuracy)\n",
    "    bar.set_description(f'(loss={vl_error})')\n",
    "mse, mee = compute_metrics(model, X_test, y_test, scaler_test)\n",
    "summary = model.summary()\n",
    "print(\"mee: \", mee, \", mse\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(path, test, mee, mse, summary):\n",
    "    iso_date = datetime.now().replace(microsecond=0).isoformat()\n",
    "    rep_test = f\"mse_test:{str(round(mse, 6))}-mee_test:{str(round(mee, 6))}\"\n",
    "    filename = f\"{path}/{iso_date}-mse_test:{str(round(mse, 6))}-mee_test:{str(round(mee, 6))}\"\n",
    "    f = open(f\"{filename}.logs\", 'w')\n",
    "    f.write(f\"{str(test)}\\n\")\n",
    "    f.write(f\"{str(rep_test)}\\n\")\n",
    "    f.write(f\"{summary}\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result(path_model_assessment_result, best_test, mee, mse, summary)\n",
    "plot_chart(path_model_assessment_result, tr_errors, vl_errors, \"Train Error\", \"Valid Error\", \"Error\")\n",
    "plot_chart(path_model_assessment_result, tr_accuracies, vl_accuracies, \"Train MEE\", \"Valid MEE\", \"MEE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(path_model_assessment_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Compute Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "problem         = \"ML-CUP23\"\n",
    "filename        = f\"datasets/ML-CUP/{problem}\"\n",
    "test            = \"-TS.csv\"\n",
    "\n",
    "def has_nan(lst):\n",
    "    return any(math.isnan(x) for x in lst)\n",
    "\n",
    "def string_dataset_to_float(dataset):\n",
    "    new_dataset = []\n",
    "    for i in range(len(dataset)):\n",
    "        if has_nan(dataset[i]) == False:\n",
    "            new_dataset.append([float(j) for j in dataset[i]])\n",
    "    return np.array(new_dataset)\n",
    "\n",
    "def retrieveData(filename, column_names, column_features):\n",
    "    data = pd.read_csv(filename, sep=',', header=None, comment=\"#\", names=column_names)\n",
    "    data=data.iloc[np.random.permutation(len(data))]\n",
    "    df_scaled = pd.DataFrame(data.to_numpy(), columns=data.columns.values)\n",
    "    df_train = df_scaled\n",
    "    X_train = df_train.iloc[ : , :].values\n",
    "    X_train = string_dataset_to_float(X_train)\n",
    "    return X_train\n",
    "\n",
    "column_names    = [\"id\", \"i1\", \"i2\", \"i3\", \"i4\", \"i5\", \"i6\", \"i7\", \"i8\", \"i9\", \"i10\"]\n",
    "column_features = [\"id\", \"i1\", \"i2\", \"i3\", \"i4\", \"i5\", \"i6\", \"i7\", \"i8\", \"i9\", \"i10\"]\n",
    "X_test = retrieveData(filename + test, column_names, column_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = MeanSquaredError()\n",
    "\n",
    "res = []\n",
    "ids = []\n",
    "for x in X_test:\n",
    "    out = model.run(x[1:])\n",
    "    res.append(out)\n",
    "    ids.append(x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team = \"team-name_ML-CUP23-TS.csv\"\n",
    "path_result = f\"results/model-assessment/ML-CUP23/{team}\"\n",
    "f = open(path_result, \"w\")\n",
    "res = scaler_test.inverse_transform(res)\n",
    "for i in range(len(res)):\n",
    "    res_str = [str(r) for r in res[i]]\n",
    "    ids_str = str(ids[i])\n",
    "    string = f\"{ids_str}, \" + \", \".join(res_str)\n",
    "    f.write(string + \"\\n\")\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
